{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "My own classifer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "NUM_BINS = 10\n",
    "SMOOTH = 1\n",
    "\n",
    "class MyMultinomialNaiveBayes:\n",
    "    def __init__(self): \n",
    "        self.prob_label = {}\n",
    "        self.prob_feature_under_label = {}\n",
    "        self.trainX = None\n",
    "        self.trainY = None\n",
    "        self.feature_cols = []\n",
    "        self.label_name = \"\"\n",
    "        self.train_step = None\n",
    "        self.train_min = None\n",
    "        self.class_labels = []\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    fit is a training process\n",
    "    '''\n",
    "    def fit(self, trainX, trainY):\n",
    "        # The first way to pick values from columns\n",
    "        # trainX = train_data[[\"Feature_1\", \"Feature_2\"]]\n",
    "        # print(trainX)\n",
    "\n",
    "        #The second way to pick values using \"iloc[]\"\n",
    "        # .iloc[ : , : 6] the first : means select all the rows, :6 means select the columns from 0 to 5.\n",
    "        # trainX = train_data.iloc[:, :6]\n",
    "        # now trainX is a dataframe.\n",
    "        # in pandas, dataframe can use matrix calculation\n",
    "\n",
    "        # the third way to pick values using \"loc[]\"\n",
    "        self.trainX = trainX\n",
    "        self.trainY = trainY\n",
    "        self.feature_cols = trainX.columns.tolist()\n",
    "\n",
    "        # initialization\n",
    "        # trainX.max()means find the max value for each column in the dataframe,\n",
    "        # when axis=0, it is finding columns max value, when axis=1, finding each row's max value\n",
    "        # print(trainX.max(axis=0)) \n",
    "        # print(trainX.min())\n",
    "        # calculate the range, divide the bin numbers and get the steps\n",
    "        self.train_min = self.trainX.min()\n",
    "        self.train_step = (self.trainX.max() - self.trainX.min()) / NUM_BINS\n",
    "        \n",
    "        # decide which bin the feature data in, // means get integers\n",
    "        self.trainX = (self.trainX - self.trainX.min()) // self.train_step\n",
    "        \n",
    "        \n",
    "        y_unique_labels = self.trainY.unique()\n",
    "        #pandas中矩阵取长度的两种方法（取rows的长度）\n",
    "        #方法一：\n",
    "        total_num_rows = len(self.trainY)\n",
    "        #方法二\n",
    "        # total_num_rows = self.trainX.shape[0]\n",
    "        \n",
    "        \n",
    "        for possible_y in y_unique_labels:\n",
    "            self.class_labels.append(possible_y)\n",
    "            #y=0时的所有行构成的矩阵，第二遍取出y=1时所有行构成的矩阵\n",
    "            df_with_possible_y = self.trainX.loc[self.trainY == possible_y, :] \n",
    "            self.prob_label[possible_y] = len(df_with_possible_y)/total_num_rows\n",
    "            \n",
    "            for col_name in self.feature_cols:\n",
    "                # find the frequence of bin number in each column(feature)\n",
    "                feature_value_counts = df_with_possible_y[col_name].value_counts()\n",
    "                #print(feature_value_counts)\n",
    "                # feature_value_counts is now a series has tow columns, sth like a dict\n",
    "                unique_feature_values = feature_value_counts.index.tolist()\n",
    "\n",
    "                for i in range(NUM_BINS):\n",
    "                    float_i = float(i)\n",
    "                    if float_i not in unique_feature_values:\n",
    "                        self.prob_feature_under_label[(possible_y, col_name, float_i)] \\\n",
    "                            = SMOOTH/(SMOOTH + len(df_with_possible_y))\n",
    "                    else:\n",
    "                        self.prob_feature_under_label[(possible_y, col_name, float_i)] = \\\n",
    "                            (feature_value_counts[float_i] + SMOOTH) / (len(df_with_possible_y) + SMOOTH)\n",
    "        # print(self.prob_feature_under_label)\n",
    "        # print(self.prob_label)\n",
    "            \n",
    "    \n",
    "    \n",
    "    def predict(self, testX):\n",
    "        df = (testX - self.train_min)//self.train_step\n",
    "        predict_y = []\n",
    "        \n",
    "        for i in range(df.shape[0]):\n",
    "            row = df.iloc[i, :]\n",
    "            max_prob = 0\n",
    "            true_y = 0\n",
    "            for possible_y in self.class_labels:\n",
    "                prob_product = 1\n",
    "                prob_c = self.prob_label[possible_y]\n",
    "                for col_name in self.feature_cols:\n",
    "                    if row[col_name] < 0:\n",
    "                        row[col_name] = 0\n",
    "                    elif row[col_name] > NUM_BINS-1:\n",
    "                        row[col_name] = NUM_BINS -1\n",
    "                    if (possible_y, col_name, row[col_name]) in self.prob_feature_under_label:\n",
    "                        prob_feature = self.prob_feature_under_label[(possible_y, col_name, row[col_name])]\n",
    "                        prob_product *= prob_feature\n",
    "                prob_product *= prob_c\n",
    "                if prob_product > max_prob:\n",
    "                    max_prob = prob_product\n",
    "                    true_y = possible_y\n",
    "            predict_y.append(true_y)\n",
    "        return predict_y\n",
    "            \n",
    "                \n",
    "\n",
    "train_data = pd.read_csv(\"hw1_trainingset.csv\")\n",
    "test_data = pd.read_csv(\"hw1_testset.csv\")\n",
    "my_nb = MyMultinomialNaiveBayes()\n",
    "my_nb.fit(train_data.iloc[:, : -1], train_data.iloc[:, -1])\n",
    "predicted_y = my_nb.predict(test_data)\n",
    "\n",
    "test_data['Label'] = predicted_y\n",
    "test_data.to_csv(\"my_multinomial_naive_bayes_predict_result.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "My own crossvalidation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The cv scores from my cross validation is [0.36950146627565983, 0.3861671469740634, 0.37362637362637363, 0.32294617563739375, 0.40816326530612246, 0.3592814371257485, 0.37777777777777777, 0.3575418994413408, 0.31671554252199413, 0.39759036144578314] and the average is 0.3669311446132258\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from cross_validation import my_cross_validation\n",
    "scores = my_cross_validation(MyMultinomialNaiveBayes, train_data.iloc[:, : -1], train_data.iloc[:, -1], 10)\n",
    "print(\"The cv scores from my cross validation is \" + str(scores) + \" and the average is \" \n",
    "      + str(sum(scores) / len(scores)) )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using sklean"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The cv scores from sklearn is [0.33819242 0.29012346 0.34582133 0.34202899 0.33898305 0.3625731\n 0.32748538 0.38692098 0.34005764 0.3253012 ] and the average is 0.3397487540783795\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train_data = pd.read_csv(\"hw1_trainingset.csv\")\n",
    "test_data = pd.read_csv(\"hw1_testset.csv\")\n",
    "\n",
    "train_data = train_data.sample(frac=1, random_state=42)\n",
    "trainX = train_data.iloc[:, : -1]\n",
    "trainY = train_data.iloc[:, -1]\n",
    "\n",
    "train_min = trainX.min()\n",
    "train_step = (trainX.max() - trainX.min()) / NUM_BINS\n",
    "trainX = (trainX - train_min) / train_step  \n",
    "        \n",
    "clf = MultinomialNB()\n",
    "clf.fit(trainX, trainY)\n",
    "\n",
    "testX = (test_data - train_min) / train_step  \n",
    "\n",
    "predicted_y = clf.predict(testX)\n",
    "test_data['Label'] = predicted_y\n",
    "test_data.to_csv(\"sklearn_multinomial_naive_bayes_predict_result.csv\")\n",
    "\n",
    "scores = cross_val_score(MultinomialNB(), trainX, trainY,\n",
    "                scoring=\"f1\",\n",
    "                cv=10)\n",
    "\n",
    "print(\"The cv scores from sklearn is \" + str(scores) + \" and the average is \" \n",
    "      + str(sum(scores) / len(scores)) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}